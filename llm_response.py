# llm_response.py

import os
from dotenv import load_dotenv
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate

# Load environment variables from .env
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    raise ValueError("Missing OpenAI API Key. Set it in the .env file.")

# Initialize the ChatOpenAI client (GPT-4o) with zero temperature for deterministic outputs
llm = ChatOpenAI(
    model_name="gpt-4o",
    temperature=0,
    openai_api_key=openai_api_key
)

# Prompt template for structured Greek responses
_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
Î•Î¯ÏƒÎ±Î¹ Î­Î½Î±Ï‚ ÎµÎ¾ÎµÎ¹Î´Î¹ÎºÎµÏ…Î¼Î­Î½Î¿Ï‚ ÏˆÎ·Ï†Î¹Î±ÎºÏŒÏ‚ Î²Î¿Î·Î¸ÏŒÏ‚ Î³Î¹Î± Ï„Î¿Î½ **Î•Î¸Î½Î¹ÎºÏŒ ÎšÎ±Ï„Î¬Î»Î¿Î³Î¿ Î¥Ï€Î·ÏÎµÏƒÎ¹ÏÎ½ Ï„Î·Ï‚ Î•Î»Î»Î¬Î´Î±Ï‚**. 
Î£Î¿Ï… Î´Î¯Î½Ï‰ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ Î±Ï€ÏŒ Ï„Î· Î“ÏÎ±Ï†Î¹ÎºÎ® Î’Î¬ÏƒÎ· Î“Î½ÏÏƒÎ·Ï‚ (GraphRAG) ÎºÎ±Î¹ Î¸Î­Î»Ï‰ Î½Î± Î±Ï€Î±Î½Ï„Î®ÏƒÎµÎ¹Ï‚ Î¼Îµ **Î´Î¿Î¼Î·Î¼Î­Î½Î¿ ÎºÎ±Î¹ ÎµÏ€Î±Î³Î³ÎµÎ»Î¼Î±Ï„Î¹ÎºÏŒ Ï„ÏÏŒÏ€Î¿ ÏƒÏ„Î± ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬**.


ğŸ”¹ **Î•ÏÏÏ„Î·ÏƒÎ·:** {question}
ğŸ”¹ **Î£Ï‡ÎµÏ„Î¹ÎºÎ­Ï‚ Î Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚:**
{context}

ğŸ”¹ **Î‘Ï€Î¬Î½Ï„Î·ÏƒÎ·:** Î”ÏÏƒÎµ Î¼Î¹Î± Î±Î½Î±Î»Ï…Ï„Î¹ÎºÎ®, ÏƒÎ±Ï†Î® ÎºÎ±Î¹ Î´Î¿Î¼Î·Î¼Î­Î½Î· Î±Ï€Î¬Î½Ï„Î·ÏƒÎ· ÏƒÏ„Î± ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬. Î•Î¾Î®Î³Î·ÏƒÎµ Ï„Î· Î´Î¹Î±Î´Î¹ÎºÎ±ÏƒÎ¯Î±, Ï„Î± Î²Î®Î¼Î±Ï„Î±, ÎºÎ±Î¹ Î±Î½ Ï…Ï€Î¬ÏÏ‡Î¿Ï…Î½ ÏƒÏ‡ÎµÏ„Î¹ÎºÎ­Ï‚ Ï…Ï€Î·ÏÎµÏƒÎ¯ÎµÏ‚.
"""
)

def generate_response(user_query: str, graph_data: list[dict], mode: str = "text_only") -> str:
    """
    Î“ÎµÎ½Î½Î¬ Î¼Î¹Î± Î±Ï€Î¬Î½Ï„Î·ÏƒÎ· ÏƒÏ„Î± ÎµÎ»Î»Î·Î½Î¹ÎºÎ¬, Ï‡ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹ÏÎ½Ï„Î±Ï‚ Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î±Ï€ÏŒ Ï„Î· Î²Î¬ÏƒÎ· Î³Î½ÏÏƒÎ·Ï‚.
    
    :param user_query: Î¤Î¿ ÎµÏÏÏ„Î·Î¼Î± Ï„Î¿Ï… Ï‡ÏÎ®ÏƒÏ„Î·.
    :param graph_data: Î›Î¯ÏƒÏ„Î± Î±Ï€ÏŒ dicts Î¼Îµ ÎºÎ»ÎµÎ¹Î´Î¹Î¬ 'node_1', 'relationship', 'node_2' (ÎºÎ±Î¹ Ï€ÏÎ¿Î±Î¹ÏÎµÏ„Î¹ÎºÎ¬ 'score').
    :param mode: "text_only" Î® "hybrid" â€” Î±Î»Î»Î¬ Î· Î¼Î¿ÏÏ†Î¿Ï€Î¿Î¯Î·ÏƒÎ· Ï€Î±ÏÎ±Î¼Î­Î½ÎµÎ¹ Î¯Î´Î¹Î±.
    :return: Î¤Î¿ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ Ï„Î·Ï‚ Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·Ï‚ Î±Ï€ÏŒ Ï„Î¿ LLM.
    """

    # ÎšÎ±Ï„Î±ÏƒÎºÎµÏ…Î¬Î¶Î¿Ï…Î¼Îµ Ï„Î¿ context Î¼Îµ Ï„Î·Î½ Î¯Î´Î¹Î± Î´Î¿Î¼Î® Î³Î¹Î± text-only ÎºÎ±Î¹ hybrid:
    context_lines = []
    for item in graph_data:
        node1 = item.get("node_1", "")
        rel   = item.get("relationship", "")
        node2 = item.get("node_2", "")
        context_lines.append(f"â¤ **{node1}** â†’ {rel} â†’ **{node2}**")
    context = "\n".join(context_lines)

    # Î“ÎµÎ¼Î¯Î¶Î¿Ï…Î¼Îµ Ï„Î¿ Ï€ÏÏŒÏ„Ï…Ï€Î¿
    prompt = _prompt.format(context=context, question=user_query)

    # ÎšÎ±Î»Î¿ÏÎ¼Îµ Ï„Î¿ LLM Î³Î¹Î± Ï€ÏÏŒÎ²Î»ÎµÏˆÎ·
    response = llm.predict(prompt)
    return response